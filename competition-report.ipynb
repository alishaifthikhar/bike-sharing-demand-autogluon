{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7525f7ad-ee40-4a07-b1f2-0a7c5bcf4590",
   "metadata": {},
   "source": [
    "# Bike Sharing Demand Prediction with AutoGluon\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This report details the process and findings of using the AutoGluon library for tabular prediction to forecast bike-sharing demand, as part of the Kaggle \"Bike Sharing Demand\" competition. The objective was to leverage AutoGluon's automated machine learning capabilities to build robust predictive models and optimize performance, ultimately aiming for a strong score on the Kaggle leaderboard.\n",
    "\n",
    "Predicting bike-sharing demand is a highly relevant problem for companies operating on-demand services, enabling them to anticipate demand fluctuations, optimize resource allocation, and enhance customer experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c79f68-c12d-473b-8311-d39854e28415",
   "metadata": {},
   "source": [
    "## 1. Dataset Loading and Initial Analysis\n",
    "\n",
    "The project began by loading the `train.csv`, `test.csv`, and `sampleSubmission.csv` datasets provided by the competition into Pandas DataFrames. The `datetime` column was parsed directly into datetime objects upon loading, ensuring proper temporal handling.\n",
    "\n",
    "Initial inspection of the datasets (`df.head()` and `df.info()`) revealed the presence of features such as `season`, `holiday`, `workingday`, `weather`, `temp`, `atemp`, `humidity`, `windspeed`, and the target variable `count` (along with `casual` and `registered` which sum up to `count` in the training data).\n",
    "\n",
    "A time-series plot of the bike rental count was generated to visualize the demand patterns over time:\n",
    "\n",
    "**Bike Rental Count Over Time**\n",
    "\n",
    "![Bike Rental Count Over Time Plot](model_train_score.png)\n",
    "\n",
    "This visualization clearly showed strong seasonality (yearly and monthly cycles) and daily patterns, including peaks during morning and evening rush hours, which are critical insights for feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d339f47d-6852-4327-ad74-de21c6692476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-17T18:39:31.789725Z",
     "iopub.status.busy": "2025-06-17T18:39:31.789454Z",
     "iopub.status.idle": "2025-06-17T18:39:31.798888Z",
     "shell.execute_reply": "2025-06-17T18:39:31.797925Z",
     "shell.execute_reply.started": "2025-06-17T18:39:31.789702Z"
    }
   },
   "source": [
    "## 2. Feature Engineering and Data Preprocessing: Discoveries and Impact on Performance\n",
    "\n",
    "Based on the initial exploratory data analysis (EDA), several key discoveries were made from the raw `datetime` column, which directly led to the engineering of new features to better capture temporal patterns. These new features were instrumental in improving model performance and contributing to the achieved Kaggle score:\n",
    "\n",
    "* **Temporal Features**: The time-series plot revealed strong periodicity. This led to the extraction of:\n",
    "    * `year`: To capture yearly trends (e.g., changes in bike usage over different years).\n",
    "    * `month`: To account for monthly variations (e.g., summer vs. winter demand).\n",
    "    * `day`: To capture daily patterns within a month.\n",
    "    * `hour`: Crucially, to identify intra-day demand spikes (e.g., rush hours).\n",
    "    * `dayofweek`: To distinguish demand on weekdays vs. weekends.\n",
    "    * `weekofyear`: To capture weekly seasonality.\n",
    "    * `is_weekend`: A binary feature derived from `dayofweek` to explicitly highlight weekend vs. weekday.\n",
    "\n",
    "* **Data Leakage Prevention**: The `casual` and `registered` columns were dropped from the training set. EDA confirmed they sum up to `count`, meaning they contain information directly from the target. Including them would have led to an artificially inflated performance on the training data and poor generalization on unseen test data, severely impacting the Kaggle score.\n",
    "\n",
    "* **Categorical Type Conversion**: Features like `season`, `holiday`, `workingday`, `weather`, and the newly extracted temporal features (`year`, `month`, `day`, `hour`, `dayofweek`, `is_weekend`, `weekofyear`) were explicitly converted to the `category` data type. This ensured AutoGluon treated them as discrete categories rather than continuous numerical values, which is appropriate for these types of features and allows models to learn distinct patterns associated with each category.\n",
    "\n",
    "* **Log Transformation of Target**: Histograms of the `count` variable showed a heavily skewed distribution (many low counts, few high counts). A **log1p transformation** (`np.log1p()`) was applied to the `count` target variable. This transformed the skewed distribution into a more symmetrical, Gaussian-like shape, which is beneficial for regression models. This directly contributed to a lower Root Mean Squared Error (RMSE) because models typically perform better when errors are more normally distributed, leading to a direct improvement in the Kaggle score. The inverse transformation (`np.expm1()`) was applied to predictions before submission.\n",
    "\n",
    "These strategic feature engineering and preprocessing steps directly provided AutoGluon's models with a richer and more appropriate representation of the data, which was essential for achieving a competitive Kaggle score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0763f7-5c57-43ba-a48a-584f58dd9672",
   "metadata": {},
   "source": [
    "## 3. Model Training with AutoGluon\n",
    "\n",
    "AutoGluon's `TabularPredictor` was used to train the predictive model. The `label` was set to `'count'`, and the `eval_metric` was specified as `'root_mean_squared_error'` (RMSE), directly matching the Kaggle competition's evaluation metric.\n",
    "\n",
    "The `predictor.fit()` method was invoked with the following key configurations:\n",
    "\n",
    "* `train_data`: The preprocessed training DataFrame.\n",
    "\n",
    "* `presets='best_quality'`: This powerful preset instructs AutoGluon to train a wide array of diverse models (e.g., LightGBM, CatBoost, XGBoost, Neural Networks, Random Forests, Extra Trees, KNN) and combine them into multi-layer stack ensembles with bagging. This strategy aims for the highest possible predictive accuracy by leveraging the strengths of different algorithms and reducing variance through ensembling.\n",
    "\n",
    "* `time_limit=3600`: A time limit of 1 hour (3600 seconds) was set for the training process.\n",
    "\n",
    "The `predictor.fit_summary()` provided a comprehensive overview of the training run, including the performance of individual base models and various ensemble levels. The `leaderboard` showed the performance of each trained model on the validation set.\n",
    "\n",
    "**Best Performing Model (Validation):**\n",
    "From the `fit_summary()` and `leaderboard()` output, which details the results of the training run:\n",
    "\n",
    "* The top-performing model on the validation set was `WeightedEnsemble_L3` with a `score_val` of **-0.254243**. This translates to a **Root Mean Squared Error (RMSE) of 0.254243** on the validation data. As indicated by AutoGluon's output, this entry represents the best model found during the training process. This ensemble model effectively combined predictions from various lower-level models to achieve the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc50bd-0533-42ec-abf7-09de68549894",
   "metadata": {},
   "source": [
    "## 4. Model Performance Comparison\n",
    "\n",
    "### 4.1. Model Training Performance (Validation RMSE)\n",
    "\n",
    "The plot below illustrates the validation RMSE of the model iterations. For this report, \"initial\" represents the performance of the `best_quality` AutoGluon ensemble after initial feature engineering and target transformation.\n",
    "\n",
    "**Model Training Scores (Validation RMSE)**\n",
    "\n",
    "![Model Training Scores (Validation RMSE)](model_train_score.png)\n",
    "\n",
    "As seen, the validation RMSE achieved by the `best_quality` AutoGluon ensemble was **0.254243**.\n",
    "\n",
    "### 4.2. Kaggle Competition Score (Public RMSE)\n",
    "\n",
    "After generating predictions on the test set, inverse transforming them, ensuring non-negativity and integer values, the `submission.csv` file was created and submitted to the Kaggle competition.\n",
    "\n",
    "**Kaggle Test Scores (RMSE)**\n",
    "\n",
    "![Kaggle Test Scores (RMSE)](model_test_score.png)\n",
    "\n",
    "The first submission to the Kaggle competition yielded a **Public Score of 0.37093**. This score reflects the model's performance on a hidden portion of the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd62d616-61ce-4365-b0cb-f7eecea20750",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning and Impact: Explaining Performance Changes\n",
    "\n",
    "AutoGluon's `best_quality` preset automates a significant amount of hyperparameter tuning and model selection, which is a major advantage for rapid development and high accuracy. Instead of manually searching for optimal hyperparameters for each algorithm, AutoGluon intelligently explores the hyperparameter space and builds powerful ensembles.\n",
    "\n",
    "The initial approach focused on a robust set of changes that directly impacted the model's performance and led to the observed Kaggle score:\n",
    "\n",
    "* **Feature Engineering**: As detailed in Section 2, the creation of specific temporal features (`year`, `month`, `day`, `hour`, `dayofweek`, `weekofyear`, `is_weekend`) provided the model with a richer understanding of the underlying patterns in bike demand. These changes affected the outcome by giving the models more direct signals related to seasonality and daily cycles, allowing them to learn more accurate relationships and reducing prediction error.\n",
    "* **Target Transformation**: The `np.log1p()` transformation of the `count` variable normalized its skewed distribution. This change significantly affected the model's performance by making the target more amenable to standard regression techniques, which often assume normally distributed errors. This led to more stable and accurate learning, resulting in a lower RMSE.\n",
    "* **AutoGluon's `best_quality` Preset (Hyperparameter Strategy)**: This preset is a high-level hyperparameter choice that embodies extensive internal tuning. It directly impacted the outcome by:\n",
    "    * **Automated Model Selection**: It intelligently selected and trained a diverse set of high-performing base models (LightGBM, CatBoost, XGBoost, Neural Networks, etc.), each with its own strengths. This ensemble diversity reduces the risk of relying on a single model's weaknesses.\n",
    "    * **Ensembling and Stacking**: It automatically created robust, multi-layer ensembles (like `WeightedEnsemble_L2` and `WeightedEnsemble_L3`). These ensembles combine the predictions of multiple models, averaging out individual model errors and leveraging their collective intelligence. This meta-learning process is a form of hyperparameter optimization itself, directly leading to better generalization and a lower RMSE on unseen data (and thus, a better Kaggle score).\n",
    "    * **Internal Hyperparameter Optimization**: Within each base model trained, AutoGluon performs its own internal, automated hyperparameter searches. This ensures that even the individual components of the ensemble are well-tuned for the given dataset and problem type.\n",
    "\n",
    "The table below outlines these key \"hyperparameter uses\" (or strategic changes) along with the Kaggle score received from this iteration:\n",
    "\n",
    "**Hyperparameter Table**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c4fc77-2fc4-4f3a-a4cf-cfbbbdb950d0",
   "metadata": {},
   "source": [
    "**Hyperparameter Table**\n",
    "\n",
    "| model   | hpo_strategy_1         | hpo_strategy_2              | hpo_strategy_3     | score   |\n",
    "| :------ | :--------------------- | :-------------------------- | :----------------- | :------ |\n",
    "| initial | AG_best_quality_preset | DateTime_Features_Extracted | Log_Transform_Target | 0.37093 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99470f1e-dc8e-401c-8bd0-28fdcca18da9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This project successfully demonstrated the application of AutoGluon for predicting bike-sharing demand. The robust feature engineering, including the extraction of temporal features and log transformation of the target, combined with AutoGluon's `best_quality` preset and its automated ensembling capabilities, resulted in a strong initial performance on the Kaggle competition with a Public Score of **0.37093**.\n",
    "\n",
    "Further improvements could be explored through:\n",
    "\n",
    "* **More Advanced Feature Engineering**: Creating additional domain-specific features (e.g., rush hour categories, temperature categories, wind/humidity categories as suggested in the project, interactions between features).\n",
    "\n",
    "* **Extended Training Time**: Allowing AutoGluon more time (`time_limit`) could lead to even better models as it could explore more complex ensembles or perform more extensive hyperparameter searches.\n",
    "\n",
    "* **Specific Model Tuning**: While `best_quality` is comprehensive, more targeted fine-tuning of specific top-performing algorithms (like LightGBM or XGBoost) or custom ensemble configurations could potentially yield marginal gains.\n",
    "\n",
    "* **Cross-Validation Strategy**: Exploring different cross-validation setups within AutoGluon if default bagging is not sufficient for certain data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c062c4dd-93de-4b59-a18f-4a159d8420a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
